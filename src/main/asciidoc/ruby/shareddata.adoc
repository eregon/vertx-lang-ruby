== Using Shared Data with Vert.x

Shared data contains functionality that allows you to safely share data between different parts of your application,
or different applications in the same Vert.x instance or across a cluster of Vert.x instances.

Shared data provides:

 * synchronous shared maps (local)
 * asynchronous maps (local or cluster-wide)
 * asynchronous locks (local or cluster-wide)
 * asynchronous counters (local or cluster-wide)

IMPORTANT: The behavior of the distributed data structure depends on the cluster manager you use. Backup
(replication) and behavior when a network partition is faced are defined by the cluster manager and its
configuration. Refer to the cluster manager documentation as well as to the underlying framework manual.

=== Local shared maps

`link:../../yardoc/Vertx/LocalMap.html[Local shared maps]` allow you to share data safely between different event
loops (e.g. different verticles) in the same Vert.x instance.

Local shared maps only allow certain data types to be used as keys and values. Those types must either be immutable,
or certain other types that can be copied like `link:../../yardoc/Vertx/Buffer.html[Buffer]`. In the latter case the key/value
will be copied before putting it in the map.

This way we can ensure there is no _shared access to mutable state_ between different threads in your Vert.x application
so you don't have to worry about protecting that state by synchronising access to it.

Here's an example of using a shared local map:

[source,ruby]
----
require 'vertx/buffer'

sd = vertx.shared_data()

map1 = sd.get_local_map("mymap1")

map1.put("foo", "bar")

map2 = sd.get_local_map("mymap2")

map2.put("eek", Vertx::Buffer.buffer().append_int(123))

# Then... in another part of your application:

map1 = sd.get_local_map("mymap1")

val = map1.get("foo")

map2 = sd.get_local_map("mymap2")

buff = map2.get("eek")

----

=== Asynchronous shared maps

Asynchronous shared maps allow data to be put in the map and retrieved locally when Vert.x is not clustered.
When clustered, data can be put from any node and retrieved from the same node or any other node.

IMPORTANT: In clustered mode, asynchronous shared maps rely on distributed data structures provided by the cluster manager.
Beware that the latency relative to asynchronous shared map operations can be much higher in clustered than in local mode.

This makes them really useful for things like storing session state in a farm of servers hosting a Vert.x web
application.

You get an instance of `link:../../yardoc/Vertx/AsyncMap.html[AsyncMap]` with
`link:../../yardoc/Vertx/SharedData.html#get_async_map-instance_method[getAsyncMap]`.

Getting the map is asynchronous and the result is returned to you in the handler that you specify. Here's an example:

[source,ruby]
----

sd = vertx.shared_data()

sd.get_async_map("mymap") { |res_err,res|
  if (res_err == nil)
    map = res
  else
    # Something went wrong!
  end
}


----

==== Putting data in a map

You put data in a map with `link:../../yardoc/Vertx/AsyncMap.html#put-instance_method[put]`.

The actual put is asynchronous and the handler is notified once it is complete:

[source,ruby]
----

map.put("foo", "bar") { |resPut_err,resPut|
  if (resPut_err == nil)
    # Successfully put the value
  else
    # Something went wrong!
  end
}


----

==== Getting data from a map

You get data from a map with `link:../../yardoc/Vertx/AsyncMap.html#get-instance_method[get]`.

The actual get is asynchronous and the handler is notified with the result some time later

[source,ruby]
----

map.get("foo") { |resGet_err,resGet|
  if (resGet_err == nil)
    # Successfully got the value
    val = resGet
  else
    # Something went wrong!
  end
}


----

===== Other map operations

You can also remove entries from an asynchronous map, clear them and get the size.

See the `link:../../yardoc/Vertx/AsyncMap.html[API docs]` for more information.

=== Asynchronous locks

`link:../../yardoc/Vertx/Lock.html[Asynchronous locks]` allow you to obtain exclusive locks locally or across the cluster -
this is useful when you want to do something or access a resource on only one node of a cluster at any one time.

Asynchronous locks have an asynchronous API unlike most lock APIs which block the calling thread until the lock
is obtained.

To obtain a lock use `link:../../yardoc/Vertx/SharedData.html#get_lock-instance_method[getLock]`.

This won't block, but when the lock is available, the handler will be called with an instance of `link:../../yardoc/Vertx/Lock.html[Lock]`,
signifying that you now own the lock.

While you own the lock no other caller, anywhere on the cluster will be able to obtain the lock.

When you've finished with the lock, you call `link:../../yardoc/Vertx/Lock.html#release-instance_method[release]` to release it, so
another caller can obtain it.

[source,ruby]
----
sd.get_lock("mylock") { |res_err,res|
  if (res_err == nil)
    # Got the lock!
    lock = res

    # 5 seconds later we release the lock so someone else can get it

    vertx.set_timer(5000) { |tid|
      lock.release()
    }

  else
    # Something went wrong
  end
}

----

You can also get a lock with a timeout. If it fails to obtain the lock within the timeout the handler will be called
with a failure:

[source,ruby]
----
sd.get_lock_with_timeout("mylock", 10000) { |res_err,res|
  if (res_err == nil)
    # Got the lock!
    lock = res

  else
    # Failed to get lock
  end
}

----

=== Asynchronous counters

It's often useful to maintain an atomic counter locally or across the different nodes of your application.

You can do this with `link:../../yardoc/Vertx/Counter.html[Counter]`.

You obtain an instance with `link:../../yardoc/Vertx/SharedData.html#get_counter-instance_method[getCounter]`:

[source,ruby]
----
sd.get_counter("mycounter") { |res_err,res|
  if (res_err == nil)
    counter = res
  else
    # Something went wrong!
  end
}

----

Once you have an instance you can retrieve the current count, atomically increment it, decrement and add a value to
it using the various methods.

See the `link:../../yardoc/Vertx/Counter.html[API docs]` for more information.